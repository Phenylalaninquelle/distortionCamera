\section{Implementierung}

Zur Simulation einer zuvor gemessenen Linsenverzerrung wurde in pbrt eine neue Kameraklasse \texttt{DistortionCamera} hinzugefügt. Die vorliegende Implementierung unterstützt drei rein radiale Verzerrungsmodelle, die von der Datenbank des Lensfun-Projektes (Version 0.3.2\footnote{Zum Zeitpunkt der Abgabe war diese die aktuellste stabile Version von Lensfun}) unterstützt werden. Lensfun ist eine in Open-Source Softwarebibliothek, die eine Datenbank vermessener Kameras und Linsen sowie Funktionen zur Korrektur von Linsenverzerrung, chromatischen Abberrationen und Vignettierung bereitstellt.

\subsection{Verzerrungsmodelle in der Lensfun-Datenbank}

Die in Lensfun implementierten Modelle bilden rein \emph{radiale} Linsenverzerrungen ab. Das heißt, dass nur Verschiebungen von bzw. zur optischen Achse der Kamera berücksichtigt werden, indem der unverzerrte Radius $r_u$ auf den verzerrten Radius $r_d$ angebildet wird. Dazu wird eine Funktion $f:[0,1] \rightarrow R$ definiert\footnote{Der tatsächliche Wertebereich der Funktionen hängt von ihren Parametern ab (siehe \ref{subsubsec:modeldef}. Diese werden durch Messung an realen Kameras ermittelt. Typische Werte sorgen dafür, dass der Wertebereich zwischen 0 und 1 liegt, was im Kontext eines normierten Radius sinnvoll ist (siehe \ref{subsubsec:norm_radius}).}. Im allgemeinen können Linsenverzerrungen auch tangentiale Komponenten enthalten, für deren Nachbildung dann komplexere Modelle nötig sind.

\subsubsection{Koordinatensystem und Normalisierung}
\label{subsubsec:norm_radius}

Der Radius wird definiert als der normierte Abstand vom Bildmittelpunkt. Dieser wird standardmäßig als $p_m = (\frac{x_{res}}{2}, \frac{y_{res}}{2})$ angenommen\footnote{In der Datenbank kann auch eine gemessene Abweichung von diesem "idealen" Bildmittelpunkt angegeben werden. Davon wird allerdings in der gesamten Datenbank, die (Stand 31.01.2019) über 600 Modelle umfasst, nur ein einziges Mal Gebrauch gemacht.}, wobei $x_{res}$ und $y_{res}$ die Bildauflösung in x- bzw. y-Richtung bezeichnen. Die Normierung erfolgt dann so, dass die halbe Bilddiagonale die Länge eins hat. Da diese den größten Radius im Bild darstellt, ist dadurch immer $r \in [0,1]$ (siehe Abbildung \ref{fig:norm}). 

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	%Bildumriss
	\draw[thick] (0,0) -- (8,0) -- (8, -4) -- (0,-4) -- (0,0);
	%Bildmitte
	\draw (4,-2) circle [radius=0.02] node[above] {$p_m$};
	%Koordinatenystem
	\draw[semithick, gray, ->] (0,0) -- (1,0) node[above] {$x$};
	\draw[semithick, gray, ->] (0,0) -- (0,-1) node[left] {$y$};
	%Eckenradius
	\draw[-{Latex[length=3mm]}] (4,-2) -- (8,0);
	%Radius Beschriftung
	\node[]	 at (6, -0.7) {$r_e$};
	%Beschriftung Bilddimensionen
	\node[left] at (0,0.2) {$(0,0)$};
	\node[left, red] at (0,-0.2) {$(0,0)$};
	\node[right] at (8,-3.8) {$(x_{res}, y_{res})$};
	\node[right, red] at (8,-4.4) {$(\frac{x_{res}}{r_e}, \frac{y_{res}}{r_e})$};
	\node at (4, -2.5) {($\frac{x_{res}}{2}, \frac{y_{res}}{2})$};
	\node[red] at (4, -3.2) {($\frac{x_{res}}{2 r_e}, \frac{y_{res}}{2 r_e})$};
	\end{tikzpicture}
	
	\caption{Verwendete Normalisierung der Bildkoordinaten nach \cite{imatest, lensfun}. Das Koordinatensystem wurde in Übereinstimmung mit \cite[S. 359]{pbrt_book} gewählt. Pixelkoordinaten in schwarz, normalisierte Koordinaten in rot.	\label{fig:normalisation}}
	\label{fig:norm}
\end{figure}

Die Transformation von Pixelkoordinaten zu normalisierten Koordinaten besteht einfach in einer Division durch $r_e$. Dies lässt sich einfach nachvollziehen, indem der Radius (in Pixelkoordinaten) für einen der Eckpunkte berechnet wird:
\begin{equation}
	r_d = \sqrt{(x_{eck} - x_{mitte})^2 + (y_{eck} - y_{mitte})^2} = \sqrt{\overline{x}^2 + \overline{y}^2}
\end{equation}
Skaliert man nun alle Koordinaten mit einem Faktor $k$, so ist der skalierte Eckenradius
\begin{equation}
	r_s = \sqrt{(k \overline{x})^2 + (k \overline{y})^2} = \sqrt{k^2 (\overline{x}^2 + \overline{y}^2)} = k r_e.
\end{equation}
Demnach ist $r_s = 1$ für $k = \frac{1}{r_e}$.

\subsubsection{Modelldefinitionen}
\label{subsubsec:modeldef}

Im folgenden steht $r_u$ immer für den normierten unverzerrten Bildradius, der durch eine Verzerrungsfunktion $f$ auf den verzerrten Radius $r_d$ abgebildet wird. Diese Bezeichnungen und die Definitionen der Verzerrungsfunktionen sind der Dokumentation von Lensfun entnommen \cite{lensfun}.

\textbf{Poly3:} Das Poly3-Modell besteht aus einem Polynom dritter Ordnung mit einem Parameter $k_1$. Es ist unabhängig von $k_1$ immer $f(0) = 0$ und $f(1) = 1$. Dadurch, dass nur ein Parameter vorhanden ist, eignet sich das Modell vornehmlich zur Modellierung weniger komplexer Verzerrungen.
\begin{equation}
	f_{poly3}(r_u) = r_u \cdot (1 - k_1 + k_1 r_u^2)
\end{equation}
\textbf{Poly5:} Dieses Modell verwendet ein Polynom fünfter Ordnung und kann dadurch komplexere Verzerrungen nachbilden als das Poly3-Modell. Es wird dafür ein weiterer Parameter benötigt. Anders als beim Poly3-Modell ist hier nicht $f(1) = 1$ garantiert.
\begin{equation}
	f_{poly5}(r_u) = r_u \cdot (1 + k_1 r_u^2 + k_2 r_u^4)
\end{equation}
\textbf{PTLens:} Diese Modell wurde aus der PTLens-Datenbank, auf der das Lensfun-Projekt aufbaut, übernommen. Das Modell besteht aus einem Polynom vierter Ordnung, mit drei Parametern $a,b,c$ und ist so aufgebaut, dass wieder die Normierung $f(1) = 1$ gilt.
\begin{equation}
	f_{ptlens}(r_u) = r_u \cdot (a r_u^3 + b r_u^2 + c r_u + 1 - a - b - c)
\end{equation}

\subsection{Implementierung}

Die Klasse \texttt{DistortionCamera} implementiert die Linsenverzeichnung. Dabei werden die Richtungen der ausgesandten Strahlen relativ zum "Referenzmodell Lochkamera" modifiziert, sodass die Verzeichnung durch die unperfekte Linse im gerenderten Bild sichtbar wird. Der Raytracer ruft zum Erzeugen der Strahlen für das Rendern eines Bildes die Klassenmethode \texttt{GenerateRay} auf. Dies passiert für jeden Pixel des zu erzeugenden Bildes mindestens einmal\footnote{Der verwendete Sampler gibt vor, wie häufig und auf welche Art und Weise für ein Pixel Strahlen generiert werden.}. Die Berechnung der Strahlrichtung in Abhängigkeit der betrachteten Pixelkoordinaten erfolgt also in \texttt{GenerateRay}.

Damit ergibt sich für die Implementierung die Anforderung, dass bei einer vorgegebenen Verzerrungsfunktion $f$ vor Beginn des Renderings die inverse Funktion bestimmt werden muss. Da für die in der Lensfun-Datenbank verwendeten Modelle keine einfache analytische Inverse gefunden werden kann, wird ein numerischer Ansatz verfolgt. Dazu werden zunächst Werte von $f$ für eine Reihe von $r_i$ im erlaubten Bereich bestimmt. Die Inverse wird dann als Polynom mittels des Least Squares Verfahrens angenähert.


\subsubsection{Least Squares}
\label{subsubsec:least}

Die Methode der kleinsten Quadrate (engl.: \emph{Least Squares}, kurz LS) stellt ein Verfahren, zu einer gegebenen Menge an Punkten eine Funktion zu finden, die eine mögliche unbekannte Gesetzmäßigkeit hinter der Lage der Punkte annähert \cite{lsq_wolfram}.

Sei $P = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$ die Menge der bekannten Punkte mit $x_i, y_i \in R$. Man nimmt nun an, dass zwischen x und y ein funktionaler Zusammenhang bestehe, der durch $m$ reelle Parameter $a_j$ bestimmt wird. Damit lässt sich $y_i = f(x_i; \overrightarrow{a})$ schreiben, wobei $\overrightarrow{a} \in R^m$ die Parameter zusammenfasst. LS versucht nun, den unbekannten Parametervektor so zu ermitteln, dass die Summe der quadratischen Abweichungen $S^2$ minimiert wird:
\begin{equation}
\begin{split}
\overrightarrow{a} = \argmin_{\overrightarrow{a} \in R^M} \{ S^2 \}, \quad
S^2 =  \sum_{i=1}^{N} (y_i - f(x_i, \overrightarrow{a}))^2
\end{split}
\label{eq:opt}
\end{equation}
Setzt man für $f$ ein Polynom vom Grad $k$ an, so ist $m = k+1$ und
%\begin{equation}
\begin{gather}
f(x_i, \overrightarrow{a}) = a_0 + a_1 x_i + \dots + a_k x_i^k = \sum_{j = 0}^{k} a_j x_i^j %\\
\end{gather}
%\end{equation}
Für das Minimum müssen die partiellen Ableitungen nach allen Parametern null sein.
\begin{equation}
\frac{\partial S^2}{\partial a_l} = -2 \sum_{i=1}^{n}(y_i - a_0 + a_1 + \dots + a_k x^k) x^l = 0, \quad 0 \leq l \leq m
\end{equation}
Es muss also ein Gleichungssystem gelöst werden, dass sich wie folgt darstellen lässt:
\begin{gather}
X^T X \cdot \overrightarrow{a} = X^T \cdot
\overrightarrow{y} \label{eq:solution}
\end{gather}
mit 
\begin{gather}
X =
\begin{bmatrix}
1 & x_1 & \dots & x_1^k \\
1 & x_2^k & \dots & x_2^k \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_n & \dots & x_n^k
\end{bmatrix}
\text{und} \overrightarrow{y} = \begin{bmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{bmatrix}
\end{gather}
Sind alle $x_i$ paarweise voneinander verschieden, so ist $X^T X$ regulär. Damit ist das Problem unter dieser Bedingung immer eindeutig lösbar. Die Matrixgleichung \ref{eq:solution} kann somit numerisch oder direkt durch Finden von $(X^T X)^{-1}$ gelöst werden \cite{lsq_poly_wolfram}. In der Implementierung von \texttt{DistortionCamera} geschieht dies mittels LR-Zerlegung (beschrieben zum Beispiel in \cite{bronstein}).

Ist also eine Funktion $f(x)$ gegeben, so kann durch Auswahl von $n$ paarweise unterschiedlichen Werten $x_i$ im Bereich $[0,1]$ die Menge $P = \{ (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) \}, \quad y_i = f(x_i)$ erzeugt werden. Um nun die inverse Funktion per LS anzunähern, tauscht man in den Paaren in $P$ nun jeweils $x_i$ und $y_i$ und führt dann das LS Verfahren wie oben beschrieben durch. Dies entspricht dem Optimierungsproblem
\begin{equation}
\begin{split}
\overrightarrow{a} = \argmin_{\overrightarrow{a} \in R^m} \{ S^2 \}, \quad
S^2 =  \sum_{i=1}^{n} (x_i - f(y_i, \overrightarrow{a}))^2\quad.
\end{split}
\end{equation}
Im Vergleich zu Gleichung \ref{eq:opt} sind also die Rollen von $x_i$ und $y_i$ getauscht.

\subsubsection{Umsetzung in C++}
Die Implementierung von \texttt{DistortionCamera} ist in den Dateien \texttt{src/camera/distortion.h} und \texttt{src/camera/distortion.cpp} enthalten. \texttt{DistortionCamera} erbt dabei von \texttt{ProjectiveCamera}.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[scale=0.8]
		\umlclass{pbrt::DistortionCamera}
		{
			+ <typedef> std::vector<pbrt::Float> coeffVec \\
			+ \underline{std::unordered\_map<std::string, int> numCoeffsForModel}\\
			- std::string distortionModel \\
		 	- pbrt::Transform Normalize \\
		 	- pbrt::Transform Denormalize \\
		 	- coeffVec coeffs \\
	 		- coeffVec fittedCoeffs
 	 	}
		{
			+ DistortionCamera \\
			+ Float GenerateRay \\
			- coeffVec InvertDistortion
			- pbrt::Point3f CalculateRayStartpoint \\
		}
	\end{tikzpicture}
	\caption{Klasse \texttt{DistortionCamera} in UML-Darstellung}
\end{figure}

Im Konstruktor der Klasse wird zunächst eine Validierung des übergebenen Verzerrungsmodell durchgeführt. Ist kein Modell in der zu rendernden Szene definiert, so wird die inverse Verzerrungsfunktion zu $f(r) = r$ gesetzt. Das Rendering entspricht dann dem einer \texttt{PerspectiveCamera}. Anschließend wird überprüft, ob das übergebende Modell implementiert ist und die korrekte Anzahl an Koeffizienten übergeben wurde. Dies geschieht anhand der Variablen \texttt{numCoeffsForModel}.
\begin{lstlisting}
auto modelIter = numCoeffsForModel.find(distortionModel);
if (modelIter == nunmCoeffsForModel.end())
	// print out error and abort rendering
\end{lstlisting}
Sind die Eingabeparameter korrekt, wird per LS die inverse Verzerrungsfunktion angenähert.
\begin{lstlisting}
fittedCoeffs = InvertDistortion(coeffs, POLY_DEGREE);
\end{lstlisting}
Außerdem wird im Konstruktor noch die Transformation \texttt{Normalize} definiert (siehe \ref{subsubsec:norm_radius} sowie die dazugehörige inverse Transformation \texttt{Denormalize}.
\begin{lstlisting}
/* -------- Define transformations for ray generation --------*/
Float xRes = film->fullResolution.x;
Float yRes = film->fullResolution.y;
Float cornerRadius = sqrt(pow(xRes, 2) + pow(yRes, 2)) / 2.;
NormalizeToCornerRadius = Scale(1. / cornerRadius, 1. / cornerRadius, 1.);
Denormalize = Inverse(NormalizeToCornerRadius);
\end{lstlisting}

Zum Auffinden der inversen Verzerrungsfunktion ist in \texttt{distortion.h} die Funktion \texttt{fitPolyCoeffs} implementiert. Diese setzt das Least Squares Verfahren wie in Abschnitt \ref{subsubsec:least} beschrieben um. Für die Matrizenoperationen wird auf Funktionen aus der Boost-Bibliothek zurückgegriffen. Boost ist eine Sammlung von Open Source C++-Blibliotheken \cite{boost}. In diesem Fall wird die Unterbibliothek \texttt{Boost::numeric::uBLAS} verwendet \cite{ublas}. BLAS steht dabei für "Basic Linear Algebra Subprograms", was Inhalt und Zielsetzung der Bibliothek gut zusammenfasst. Zusätzlich ist eine Funktion \texttt{evalPolynomial} vorhanden, die für einen gegebenen Satz an $m$ Polynomkoeffizienten $a_i$ und einen Wert $x \in R$ die Funktion $y = a_0 + a_1 x + a_2 x^2 + \dots + a_{m-1} x^{m-1}$ berechnet. In \texttt{InvertDistortion} wird die übergebene Verzerrrungsfunktion abgetastet und anschließend der Koeffizientenvektor der inversen Funktion genähert.
\begin{lstlisting}
// fill sample vector according to the model used
for (int i = 0; i < sampleSize; i++) {
	x[i] = i / scale;
	y[i] = (*modelFunc)(x[i], coeffs);
}
coeffVec polyCoeffs = fitPolyCoeffs(y, x, polyDegree);
\end{lstlisting}

Zur Erzeugung des Strahls für ein vom Samples übergenes \texttt{CameraSample} wird dann \texttt{CalculateRayStartpoint} in \texttt{GenerateRay} aufgerufen. Die Funktion unterscheidet sich von ihrem Äquivalent in \texttt{PerspectiveCamera} nur durch die folgende Zeile:
\begin{lstlisting}
Point3f pCamera = RasterToCamera(CalculateRayStartpoint(sample));
\end{lstlisting}

Neben dieser eigentlichen Implementierung der neuen Kameraklasse waren noch geringe Anpassungen nötig, um das Erzeugen einer \texttt{DistortionCamera} aus einer \texttt{.pbrt}-Datei heraus zu ermöglichen. Diese betrafen die Funktion \texttt{MakeCamera} in \texttt{/src/core/api.cpp}. Außerdem musste die neue Abhängigkeit von der Boost-Bibliothek in \texttt{CMakeLists.txt} eingepflegt werden.